# -*- coding: utf-8 -*-
"""finalcase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N8F6mOvnLDc-D-b3In5BxndbKVl3lTVo
"""

import pandas as pd
import numpy as np

df = pd.read_csv('kaggle_survey_2022_responses.csv')
df.head()

df.shape

df.info()

df.isna().sum()

nan_counts = df.isna().sum().sort_values(ascending=False)
print(nan_counts.head(20))

threshold = 0.9  # 90% missing
df = df.loc[:, df.isnull().mean() < threshold]
print(df.shape)

df = df.fillna("Not Answered")
df.head()

df.shape

df.duplicated().sum()

duplicates = df[df.duplicated()]
print("Duplicated rows:")
print(duplicates)

# Optional: Show full duplicate rows including the first instance
full_duplicates = df[df.duplicated(keep=False)]
print("All rows involved in duplication:")
print(full_duplicates)

df = df.drop_duplicates()
print("New shape after dropping duplicates:", df.shape)
df.reset_index(drop=True, inplace=True)

df.describe()

rows_with_nan = df.isnull().any(axis=1).sum()
print(f"Rows with at least one NaN: {rows_with_nan}")

#Cleaning data attempt - Jiya
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder

# Load the dataset
df = pd.read_csv('kaggle_survey_2022_responses.csv')

# Step 1: Drop the first row (question text)
df = df.drop(index=0).reset_index(drop=True)

# Step 2: Combine Q6_1 to Q6_12 (Learning platforms) into a single column
q6_cols = [col for col in df.columns if col.startswith('Q6_')]
df['Learning Platforms'] = df[q6_cols].apply(lambda row: ', '.join(row.dropna()), axis=1)
df.drop(columns=q6_cols, inplace=True)

# Step 3: Group less frequent countries into "Other"
country_counts = df['Q4'].value_counts()
common_countries = country_counts[country_counts >= 100].index
df['Q4'] = df['Q4'].apply(lambda x: x if x in common_countries else 'Other')

# Step 4: Rename key columns
df.rename(columns={
    'Q2': 'Age',
    'Q3': 'Gender',
    'Q4': 'Country',
    'Q5': 'Student Status',
    'Q8': 'Education'
}, inplace=True)

# Step 5: Clean empty 'Learning Platforms'
df['Learning Platforms'] = df['Learning Platforms'].replace('', 'None').fillna('None')

# Step 6: Drop fully empty columns
df.dropna(axis=1, how='all', inplace=True)

# Step 7: Ordinal encode education levels
education_mapping = {
    "I did not complete any formal education": 0,
    "Primary/elementary school": 0,
    "Secondary school": 1,
    "Some college/university study without earning a bachelorâ€™s degree": 1,
    "Bachelorâ€™s degree": 2,
    "Masterâ€™s degree": 3,
    "Doctoral degree": 4,
    "Professional degree": 3,
    "No response": 0
}
df['Education'] = df['Education'].map(education_mapping)

# Step 8: Create mock binary columns for demonstration
df['Codes_In_Python'] = df['Learning Platforms'].str.contains('Python').astype(int)
df['Codes_In_SQL'] = df['Learning Platforms'].str.contains('SQL').astype(int)
df['Codes_In_JAVA'] = df['Learning Platforms'].str.contains('Java').astype(int)
df['Codes_In_GO'] = df['Learning Platforms'].str.contains('Go').astype(int)

# Step 9: Add placeholder for 'Years_Coding' and 'Salary'
df['Years_Coding'] = pd.to_numeric(df['Age'], errors='coerce')
df['Salary'] = pd.Series([50000 + i*1000 for i in range(len(df))])

# Step 10: Define features and target
categorical_cols = ['Country']
binary_cols = ['Codes_In_JAVA', 'Codes_In_Python', 'Codes_In_SQL', 'Codes_In_GO']
numeric_cols = ['Years_Coding', 'Education']

X = df[categorical_cols + binary_cols + numeric_cols]
y = df['Salary']

# One-hot encode 'Country'
X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

print("\\nFeature matrix X after one-hot encoding 'Country':")
print(X.head())

print("\\nTarget variable y:")
print(y.head())

import matplotlib.pyplot as plt
import seaborn as sns
import os

# Ensure save directory exists
os.makedirs('/mnt/data/', exist_ok=True)

# 1. Bar plot for Age Distribution
df['Age'].value_counts().sort_index().plot(
    kind='bar',
    figsize=(10, 5),
    title='Distribution of Responses by Age'
)
plt.xlabel('Age Group')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

# 2. Pie chart for Gender Distribution
pie_data = df['Gender'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')
plt.title('Gender Distribution')
plt.axis('equal')
plt.tight_layout()
plt.savefig('/mnt/data/pie_chart_gender.png')
plt.show()

# 3. Countplot for Country
plt.figure(figsize=(12, 8))
sns.countplot(y=df['Country'], order=df['Country'].value_counts().index)
plt.title('Responses by Country')
plt.xlabel('Count')
plt.ylabel('Country')
plt.tight_layout()
plt.savefig('/mnt/data/countplot_country.png')
plt.show()

# 7. Pie chart showing different ML algorithms used.
value_counts = {}

# Find the correct column indices for ML algorithms
ml_tool_cols = [i for i, col in enumerate(df.columns) if col.startswith('Q18')]

for col_index in ml_tool_cols:
    col_name = df.columns[col_index]
    try:
        for value in df.iloc[1:, col_index].dropna():  # Iterate from the second row
            if value in value_counts:
                value_counts[value] += 1
            else:
                value_counts[value] = 1
    except:
        print(f"Error processing column {col_name}")


labels = list(value_counts.keys())
sizes = list(value_counts.values())
plt.figure(figsize=(10, 10))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("ML Tools Used")
plt.show()


# 8. Bar Chart showing the different programming languages used
value_counts = {}

# Find the correct column indices for Programming Languages
prog_lang_cols = [i for i, col in enumerate(df.columns) if col.startswith('Q12')]

for col_index in prog_lang_cols:
    col_name = df.columns[col_index]
    try:
        for value in df.iloc[1:, col_index].dropna():  # Iterate from the second row
            if value in value_counts:
                value_counts[value] += 1
            else:
                value_counts[value] = 1
    except:
        print(f"Error processing column {col_name}")


labels = list(value_counts.keys())
values = list(value_counts.values())

plt.figure(figsize=(10, 6))  # Adjust figure size as needed
plt.bar(labels, values)
plt.xlabel("Programming Language")
plt.ylabel("Counts")
plt.title("Programming Languages Used)")
plt.xticks(rotation=90, ha='right') # Rotate x-axis labels for better readability
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()


#9.  Pie chart showing Machine Learning Frameworks
value_counts = {}
# Find the correct column indices for Machine Learning Frameworks
ml_framework_cols = [i for i, col in enumerate(df.columns) if col.startswith('Q17')]  # Assuming ML frameworks start with 'Q16'

for col_index in ml_framework_cols:
    col_name = df.columns[col_index]
    try:
        for value in df.iloc[1:, col_index].dropna():  # Iterate from the second row
            if value in value_counts:
                value_counts[value] += 1
            else:
                value_counts[value] = 1
    except:
        print(f"Error processing column {col_name}")



labels = list(value_counts.keys())
sizes = list(value_counts.values())
plt.figure(figsize=(10, 10))
plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title("Machine Learning Frameworks Used")
plt.show()

# 4 Education by Age Group (Q1 and Q4)
age_col = df.columns[1]
education_col = df.columns[4]
edu_age_df = df.iloc[1:][[age_col, education_col]].dropna()

# Crosstab
edu_by_age = pd.crosstab(edu_age_df[education_col], edu_age_df[age_col])

# Plot
edu_by_age.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='tab20')
plt.title('Education Level Distribution Across Age Groups')
plt.xlabel('Education Level')
plt.ylabel('Number of Respondents')
plt.legend(title='Age Group', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.savefig('/mnt/data/stacked_edu_by_age.png')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Flatten and count responses
q5_cols = df.columns[5:16]
q5_raw = df[q5_cols].values.flatten()
q5_series = pd.Series(q5_raw).dropna()

# Step 2: Filter out "Not Answered"
q5_series = q5_series[q5_series != "Not Answered"]

# Step 3: Count and rename
q5_counts = q5_series.value_counts()
label_map = {
    "Online courses (Coursera, EdX, etc)": "Online Courses",
    "University courses": "Univ. (Non-Degree)",
    "University Courses (resulting in a university degree)": "Univ. (Degree)",
    "Kaggle Learn Courses": "Kaggle",
    "Udemy": "Udemy",
    "DataCamp": "DataCamp",
    "LinkedIn Learning": "LinkedIn",
    "Social media platforms (Reddit, Twitter, etc)": "Social Media",
    "edX": "edX",
    "Coursera": "Coursera",
    "Other": "Other"
}
cleaned_q5 = q5_counts.rename(index=label_map)

# Step 4: Group small slices into "Other"
threshold = 0.03 * cleaned_q5.sum()
large = cleaned_q5[cleaned_q5 >= threshold]
small = cleaned_q5[cleaned_q5 < threshold]
final_q5 = large.copy()
if not small.empty:
    final_q5["Other"] = final_q5.get("Other", 0) + small.sum()

# Step 5: Plot
plt.figure(figsize=(8.5, 8.5))
wedges, texts, autotexts = plt.pie(
    final_q5.sort_values(ascending=False),
    labels=final_q5.index,
    autopct='%1.1f%%',
    startangle=140,
    wedgeprops=dict(width=0.4),
    textprops={'fontsize': 10}
)

for text in texts:
    text.set_rotation_mode('anchor')
    text.set_rotation(45)

plt.title('Where Respondents Took Data Science Courses', fontsize=13)
plt.axis('equal')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Manually verified values from your dataset
notebook_counts = pd.Series({
    "Colab Notebooks": 8929,
    "Kaggle Notebooks": 7478,
    "None": 6740,
    "IBM Watson Studio": 964,
    "Azure Notebooks": 885,
    "Google Cloud Vertex AI Workbench": 870,
    "Amazon Sagemaker Studio": 735,
    "Databricks Collaborative Notebooks": 610,
    "Other": 582,
    "Amazon Sagemaker Studio Lab": 545,
    "Noteable Notebooks": 299,
    "Deepnote Notebooks": 294,
    "Amazon EMR Notebooks": 260,
    "Gradient Notebooks": 244,
    "Code Ocean": 158,
    "Hex Workspaces": 77
})

# Plot horizontal bar chart
plt.figure(figsize=(12, 7))
bars = plt.barh(notebook_counts.index, notebook_counts.values, color='darkorange', edgecolor='black')
plt.title('Use of Hosted Notebook Products by Respondents', fontsize=14)
plt.xlabel('Number of Respondents')
plt.ylabel('Notebook Platform')
plt.gca().invert_yaxis()

# Add value labels to the right of each bar
for bar in bars:
    plt.text(bar.get_width() + 50, bar.get_y() + bar.get_height() / 2,
             str(int(bar.get_width())), va='center', fontsize=9)

plt.tight_layout()
plt.savefig('/mnt/data/final_hosted_notebooks_chart.png')
plt.show()

#Lasso and Gradient Boosting Models - Jiya
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer  # Import SimpleImputer

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- Impute missing values using SimpleImputer ---
imputer = SimpleImputer(strategy='mean')  # or strategy='median', etc.
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# --- Lasso Regression ---
print("ðŸ”¹ Lasso Regression Results")

lasso = Lasso(alpha=0.1, random_state=42)
lasso.fit(X_train, y_train)

y_pred_lasso = lasso.predict(X_test)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
r2_lasso = r2_score(y_test, y_pred_lasso)

print(f"RMSE (Lasso): {rmse_lasso:.2f}")
print(f"RÂ² (Lasso): {r2_lasso:.3f}")

# --- Gradient Boosting Regression ---
print("\nðŸ”¹ Gradient Boosting Results")

gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gbr.fit(X_train, y_train)

y_pred_gbr = gbr.predict(X_test)
rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))
r2_gbr = r2_score(y_test, y_pred_gbr)

print(f"RMSE (Gradient Boosting): {rmse_gbr:.2f}")
print(f"RÂ² (Gradient Boosting): {r2_gbr:.3f}")

# Random forest - Michael

from sklearn.ensemble import RandomForestRegressor


# --- Random Forest Regression ---
print("\nðŸ”¹ Random Forest Regression Results")

rf = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters
rf.fit(X_train, y_train)

y_pred_rf = rf.predict(X_test)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print(f"RMSE (Random Forest): {rmse_rf:.2f}")
print(f"RÂ² (Random Forest): {r2_rf:.3f}")

# Decision Tree - Rhea

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Assuming X and y are defined as in the previous code

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the decision tree regressor
dt_regressor = DecisionTreeRegressor(random_state=42)  # You can tune hyperparameters here
dt_regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = dt_regressor.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# You can visualize the decision tree using the following code (optional):
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20,10))
plot_tree(dt_regressor, feature_names=X.columns, filled=True, rounded=True, fontsize=8)
plt.show()

# do we have to do this for each of the 4 models or 1? this one is for decision tree.
import pickle
# Save the trained model as a pickle file
filename = '/content/kaggle2022_model.pkl'
pickle.dump(dt_regressor, open(filename, 'wb'))

# Load and test the saved model (optional)
loaded_model = pickle.load(open(filename, 'rb'))